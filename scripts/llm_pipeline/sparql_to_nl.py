import os
import time
import sys
import pandas as pd
from datetime import datetime
import openai
from dotenv import load_dotenv
from tqdm import tqdm
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("processing.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

load_dotenv()

# Configuration
CONFIG = {
    "temperature": 0.0,
    "max_tokens": 4096,
    "top_p": 1.0,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0,
    "checkpoint_frequency": 50,
    "api_delay": 0.1,
    "max_retries": 3,
    "retry_delay": 2.0,
}

# Set your API key
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables")

# Initialize OpenAI client
client = openai.OpenAI(api_key=openai_api_key)

# Prompt templates
PROMPT_TEMPLATES = {
    "binary": """Assume you have to rephrase the following SPARQL query as a true/false (binary) natural-language question for non-technical humans. Do not include birth dates or birth years in the question.
Use the {language_string} language. SPARQL query: 
{sparql}""",
    "membership": """Assume you have to rephrase the following SPARQL query as a multiple or single choice natural-language question for non-technical humans. Avoid technical phrases like 'type of entity' or 'rdf:type'. Ask what class or classes the individual belongs to. Do not include birth dates or birth years in the question. 
Use the {language_string} language. SPARQL query:
{sparql}""",
    "property": """Assume you have to rephrase the following SPARQL query as a multiple or single choice natural-language question for non-technical humans. Describe the relationship or attributes involved, and avoid SPARQL or ontology jargon. Do not include birth dates or birth years in the question.
Use the {language_string} language. SPARQL query:
{sparql}""",
}


def get_model_version_info(response):
    """Extract model version information from response."""
    model_version = "Unknown"
    if hasattr(response, "model"):
        model_version = response.model
    if hasattr(response, "system_fingerprint"):
        model_version += f" (fingerprint: {response.system_fingerprint})"
    return model_version


def get_prompt_template(answer_type, task_type=None):
    """Get appropriate prompt template based on answer type and task type."""
    if answer_type == "Binary":
        return PROMPT_TEMPLATES["binary"]
    elif answer_type == "Multi Choice":
        if task_type and task_type.strip() == "Membership":
            return PROMPT_TEMPLATES["membership"]
        else:
            return PROMPT_TEMPLATES["property"]
    else:
        raise ValueError(f"Unknown Answer Type: {answer_type}")


def make_api_call_with_retry(client, messages, gen_params, max_retries=3):
    """Make API call with retry logic."""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini", messages=messages, **gen_params
            )
            return response.choices[0].message.content, get_model_version_info(response)
        except Exception as e:
            logger.warning(f"API call attempt {attempt + 1} failed: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(CONFIG["retry_delay"] * (attempt + 1))
            else:
                logger.error(f"All {max_retries} API call attempts failed")
                return f"[ERROR] {str(e)}", "Error - Unable to retrieve"


def validate_dataframe(df):
    """Validate input DataFrame has required columns."""
    required_columns = ["SPARQL Query"]
    missing_cols = [col for col in required_columns if col not in df.columns]
    if missing_cols:
        raise ValueError(f"CSV must contain columns: {missing_cols}")

    df["Language"] = df.get("Language", "English")
    df["Answer Type"] = df.get("Answer Type", "Binary")

    logger.info(f"Loaded DataFrame with {len(df)} rows")
    return df


def save_checkpoint(df, logs, timestamp, checkpoint_num):
    """Save intermediate results."""
    checkpoint_dir = Path(f"checkpoints_{timestamp}")
    checkpoint_dir.mkdir(exist_ok=True)

    df.to_csv(
        checkpoint_dir / f"checkpoint_{checkpoint_num}_questions.csv", index=False
    )
    pd.DataFrame(logs).to_csv(
        checkpoint_dir / f"checkpoint_{checkpoint_num}_log.csv", index=False
    )
    logger.info(f"Checkpoint {checkpoint_num} saved")


def create_output_directory(timestamp):
    """Create organized output directory."""
    output_dir = Path(f"2hop_family_output_{timestamp}")
    output_dir.mkdir(exist_ok=True)
    return output_dir


def main():
    # Load and validate data
    try:
        df = pd.read_csv("../SPARQL_questions_sampling_2hop.csv")
        df = validate_dataframe(df)
    except FileNotFoundError:
        logger.error("Input CSV file not found")
        return
    except Exception as e:
        logger.error(f"Error loading CSV: {str(e)}")
        return

    # Setup output
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = create_output_directory(timestamp)
    logs = []

    # Track statistics
    stats = {"total": len(df), "processed": 0, "errors": 0, "start_time": time.time()}

    # Process each SPARQL question with progress bar
    for idx, row in tqdm(
        df.iterrows(), total=len(df), desc="Processing SPARQL queries"
    ):
        try:
            sparql = row["SPARQL Query"]
            language_string = row.get("Language", "English")
            answer_type = row.get("Answer Type", "Binary")
            task_type = row.get("Task Type", "")

            # Get appropriate prompt
            prompt_template = get_prompt_template(answer_type, task_type)
            prompt = prompt_template.format(
                language_string=language_string, sparql=sparql
            )

            print(f"\nðŸ”Ž Processing question #{idx + 1} in {language_string}")

            start_time = time.time()
            messages = [{"role": "user", "content": prompt}]
            gen_params = {
                k: v
                for k, v in CONFIG.items()
                if k
                in [
                    "temperature",
                    "max_tokens",
                    "top_p",
                    "presence_penalty",
                    "frequency_penalty",
                ]
            }

            # Make API call with retry
            content, model_version = make_api_call_with_retry(
                client, messages, gen_params, CONFIG["max_retries"]
            )

            end_time = time.time()

            # Show result
            if content.startswith("[ERROR]"):
                print(f"âŒ Error: {content}")
                stats["errors"] += 1
            else:
                print(f"âœ… Generated Question: {content}")
                stats["processed"] += 1

            # Save response to DataFrame
            df.at[idx, "Question"] = content

            # Create comprehensive log entry
            log_entry = {
                "SPARQL_index": idx,
                "SPARQL_query": sparql,
                "generated_question": content,
                "language": language_string,
                "answer_type": answer_type,
                "task_type": task_type,
                "model": "gpt-4o-mini",
                "model_version": model_version,
                "python_version": sys.version.split()[0],
                "timestamp_request": datetime.now().isoformat(),
                "response_time_sec": round(end_time - start_time, 3),
                "response_preview": content[:100],
                "success": not content.startswith("[ERROR]"),
            }

            log_entry.update(gen_params)
            logs.append(log_entry)

            # Rate limiting
            time.sleep(CONFIG["api_delay"])

        except Exception as e:
            logger.error(f"Error processing row {idx}: {str(e)}")
            stats["errors"] += 1

        # Save checkpoint
        if (idx + 1) % CONFIG["checkpoint_frequency"] == 0:
            save_checkpoint(
                df, logs, timestamp, (idx + 1) // CONFIG["checkpoint_frequency"]
            )

    # Save final outputs
    output_filename = output_dir / "generated_questions.csv"
    log_filename = output_dir / "processing_log.csv"
    stats_filename = output_dir / "processing_stats.txt"

    df.to_csv(output_filename, index=False)
    pd.DataFrame(logs).to_csv(log_filename, index=False)

    # Save processing statistics
    total_time = time.time() - stats["start_time"]
    with open(stats_filename, "w") as f:
        f.write(f"Processing Statistics\n")
        f.write(f"====================\n")
        f.write(f"Total questions: {stats['total']}\n")
        f.write(f"Successfully processed: {stats['processed']}\n")
        f.write(f"Errors: {stats['errors']}\n")
        f.write(f"Success rate: {stats['processed'] / stats['total'] * 100:.1f}%\n")
        f.write(f"Total processing time: {total_time:.2f} seconds\n")
        f.write(
            f"Average time per question: {total_time / stats['total']:.2f} seconds\n"
        )

    print(f"\nâœ… Done! Results saved to {output_dir}/")
    print(
        f"ðŸ“Š Success rate: {stats['processed']}/{stats['total']} ({stats['processed'] / stats['total'] * 100:.1f}%)"
    )

    logger.info(
        f"Processing completed. Success rate: {stats['processed']}/{stats['total']}"
    )


if __name__ == "__main__":
    main()
